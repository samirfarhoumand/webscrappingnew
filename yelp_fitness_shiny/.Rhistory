round(model$residuals, 2),
cex = 0.5)
#The largest residual is from a cat with both the largest body and heart weights;
#its residual value is about 5.12. This doesn't appear to be too concerning at
#the moment, even though this value might be considered an outlier.
#-----------------with ggplot----------------------
ggplot(data = cats,aes(x = Bwt,y = Hwt)) +
geom_point() +
stat_smooth(method = "lm", col = "red",se=F)+
geom_segment(aes(x = Bwt,
xend = Bwt,
y = Hwt,
yend = (model$coefficients[1] +
model$coefficients[2]*cats$Bwt)),
color = "blue")
#---------------------------------------------------
#c confidence intervals
confint(model)
#We are 95% confident that the average heart weight of a cat with
#no body mass is within (-1.73 g, 1.01 g).
#We are 95% confident that the average change in heart weight of
#a cat as its body mass increases by 1 kg is within (3.54 g, 4.53 g).
newdata = data.frame(Bwt = seq(1.9, 4, length.out = 100))
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
plot(cats$Bwt,
cats$Hwt,
xlab = "Body Weight (kg)",
ylab = "Heart Weight (g)",
main = "Anatomical Data from Domestic Cats")
abline(model, lty = 2)                              #Plotting the regression line.
lines(newdata$Bwt, conf.band[, 2], col = "blue")    #Plotting the lower confidence band.
lines(newdata$Bwt, conf.band[, 3], col = "blue")    #Plotting the upper confidence band.
lines(newdata$Bwt, pred.band[, 2], col = "red")     #Plotting the lower prediction band.
lines(newdata$Bwt, pred.band[, 3], col = "red")     #Plotting the upper prediction band.
legend("topleft",
c("Reg. Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1),
col = c("black", "blue", "red"))
#The confidence band is tighter than the prediction band because
#it assesses the average heart weight for a given body weight;
#the prediction band only assess a single instance of body weight.
#The certainty of the confidence band becomes lower as we travel
#away from the heart of the data because there are fewer observations
#upon which to base our local estimation.
newdata = data.frame(Bwt = c(2.8, 5, 10))
predict(model, newdata, interval = "confidence")
predict(model, newdata, interval = "prediction")
#In general, predictions for observations outside the realm of our data are not
#good practice; it is difficult to extrapolate to a population on which we have
#no data. For example, here we attempt to generalize to cats with body weights
#of 10 kg, yet this is not nearly contained in the range of our dataset.
bc = boxcox(model)
lambda = bc$x[which(bc$y == max(bc$y))]
lambda
Hwt.bc = log(cats$Hwt)
cats.bc = data.frame(Hwt = Hwt.bc, Bwt = cats$Bwt)
model.bc = lm(Hwt~Bwt, data = cats.bc)
summary(model.bc)
plot(model.bc)
#boxcox plot
bc = boxcox(model)
#choose lambda
lambda = bc$x[which(bc$y == max(bc$y))]
lambda
#While the lambda value that maximizes the log-likelihood function
#is approximately 0.101, this is extremely close to 0 (which is
#also contained within the 95% confidence interval). For the
#purposes of balancing interpretation and accuracy, we will move
#forward with a log transformation of our data.
Hwt.bc = log(cats$Hwt)
#Try to balance interpretability and accuracy; when taking this
#perspective, there is not a completely correct answer
#new model
cats.bc = data.frame(Hwt = Hwt.bc, Bwt = cats$Bwt)
model.bc = lm(Hwt~Bwt, data = cats.bc)
#interpret results
summary(model.bc)
plot(model.bc)
#There doesn't appear to be any violated assumptions in the new model.
#Both coefficients in the model are significant.
plot(cats$Bwt,
Hwt.bc,
xlab = "Body Weight (kg)",
ylab = "Log Heart Weight (g)",
main = "Anatomical Data from Domestic Cats\nBox-Cox Transformed")
abline(model.bc, lty = 2)
shiny::runApp()
runApp()
c
runApp()
##################################
#####Visualizing Missing Data#####
##################################
library(VIM) #For the visualization and imputation of missing values.
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
VIM::aggr(sleep) #A graphical interpretation of the missing values and their
library(mice) #Load the multivariate imputation by chained equations library.
dim(sleep)
is.na(sleep)
sum(is.na(sleep))
?md.pattern
mice::md.pattern(sleep) #Can also view this information from a data perspective.
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
#Mean value imputation method 2.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
missing.data
#Mean value imputation method 3.
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = caret::preProcess(missing.data, method = "medianImpute")
missing.data = predict(pre, missing.data)
missing.data
View(pre)
### Why Caret?
## 1. Maintain the structure of train - predict as other machine learning procedure.
##    This is particularly important when impute for future observation
## 2. Can be collected with other preprocesses, as below:
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("center","scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
## manual scale
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
scaled = mapply(FUN = '/',missing.data,sapply(missing.data, function(x) {sd(x,na.rm=T)}))
scaled
#Mean value imputation method 4.
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
library(RANN)
install.packages("RANN")
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
library(RANN)
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
#Imputing using 5NN.
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
#Imputing using 9NN.
pre.9nn = preProcess(missing.data, method = 'knnImpute', k=9)
imputed.9nn = predict(pre.9nn, missing.data)
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
#Using 8 nearest neighbors.
pre.8nn = preProcess(sleep, method = 'knnImpute', k=8)
sleep.imputed8NN = predict(pre.8nn, sleep)
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
#K-Nearest Neighbors classification on the iris dataset.
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(PASWR)
summary(titanic3)
colnames(titanic3)
library(PASWR)
is.na(titanic3)
library(PASWR)
sum(is.na(titanic3))
library(PASWR)
colSums(is.na(titanic3))
library(PASWR)
colSums(is.na(titanic3))/nrow(titanic3)
library(PASWR)
colSums(is.na(titanic3))/nrow(titanic3)*100
library(PASWR)
colnames(is.na(titanic3))
colSums(is.na(titanic3))/nrow(titanic3)*100
complete.cases(titanic3)
nrow(complete.cases(titanic3))
sum(complete.cases(titanic3))
library(PASWR)
str(titanic3)
colSums(is.na(titanic3))/nrow(titanic3)*100
library(PASWR)
str(titanic3)
dim(titanic3)
summary(titanic3)
colSums(is.na(titanic3))/nrow(titanic3)*100
sum(!complete.cases(titanic3))
sum(!complete.cases(titanic3))
sum(!complete.cases(titanic3))/nrow(titanic3)*100
sum(is.na(titanic3))
sum(is.na(titanic3))
dim(titanic3)[1]
sum(is.na(titanic3))
dim(titanic3)[1] * dim(titanic3)[2]
sum(is.na(titanic3))
obs = dim(titanic3)[1] * dim(titanic3)[2]
sum(is.na(titanic3))/obs
sum(is.na(titanic3))
obs = dim(titanic3)[1] * dim(titanic3)[2]
sum(is.na(titanic3))/obs * 100
library(VIM)
library(VIM)
VIM::aggr(titanic3)
library(VIM)
VIM::aggr(titanic3)
library(mice)
library(VIM)
VIM::aggr(titanic3)
library(mice)
mice::md.pattern(titanic3)
table(titanic3$survived, is.na(titanic3$body))
hist(titanic3$age)
library(caret)
caret::preProcess(titanic3, method = "meanImpute")
library(caret)
caret::preProcess(titanic3, method = "medianImpute")
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
predict(pre, titanic3)
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
caret.titanic = predict(pre, titanic3)
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
caret.titanic = predict(pre, titanic3)
hist(titanic3$age)
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
caret.titanic = predict(pre, titanic3)
hist(titanic3$age)
hist(caret.tianic$age)
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
caret.titanic = predict(pre, titanic3)
hist(titanic3$age)
hist(caret.titanic$age)
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
caret.titanic = predict(pre, titanic3)
hist(titanic3$age)
hist(caret.titanic$age)
library(Hsmic)
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
caret.titanic = predict(pre, titanic3)
hist(titanic3$age)
hist(caret.titanic$age)
library(Hmisc)
imputed.age = impute(titanic3$age, mean)
library(caret)
pre = caret::preProcess(titanic3, method = "medianImpute")
caret.titanic = predict(pre, titanic3)
hist(titanic3$age)
hist(caret.titanic$age)
library(Hmisc)
imputed.age = Hmisc::impute(titanic3$age, mean)
hist(imputed.age)
imputed.age = Hmisc::impute(titanic3$age, "random")
hist(imputed.age)
imputed.age = Hmisc::impute(titanic3$age, "random")
hist(imputed.age)
hist(titanic3$age)
imputed.fare = Hmisc::impute(titanic3$fare, "random")
imputed.fare = Hmisc::impute(titanic3$fare, "random")
imputed.fare$imputed
imputed.fare = Hmisc::impute(titanic3$fare, "random")
summary(imputed.fare)
imputed.fare = Hmisc::impute(titanic3$fare, "random")
summary(imputed.fare)
imputed.fare = Hmisc::impute(titanic3$fare, "random")
summary(imputed.fare)
imputed.fare = Hmisc::impute(titanic3$fare, "random")
summary(imputed.fare)
imputed.fare = Hmisc::impute(titanic3$fare, "random")
summary(imputed.fare)
imputed.fare = Hmisc::impute(titanic3$fare, "random")
summary(imputed.fare)
plot(imputed.fare, imputed.age, col = pclass)
plot(imputed.fare, imputed.age, col = titanic$pclass)
plot(imputed.fare, imputed.age, col = titanic3$pclass)
plot(imputed.age, imputed.fare, col = titanic3$pclass)
new.people = data.frame(age = c(50, 10), fare = c(400, 100), pclass = NA)
new.people = data.frame(age = c(50, 10), fare = c(400, 100), pclass = NA)
titanic4 = rbind(titanic3, new.people)
col.vec[titanic3$pclass == "1st"] = "red"
col.vec = NA
col.vec[titanic3$pclass == "1st"] = "red"
col.vec[titanic3$pclass == "2nd"] = "blue"
col.vec[titanic3$pclass == "3rd"] = "green"
plot(imputed.age, imputed.fare, col = col.vec)
plot(imputed.age, imputed.fare, col = col.vec)
new.people = data.frame(age = c(50, 10), fare = c(400, 100), pclass = NA)
points(new.people, pch = 16)
titanic.imputed = data.frame(age = imputed.age,
fare = imputed.fare,
pclass = titanic3$pclass)
imputed.1nn = kNN(missing.data, k=1)
titanic.imputed = data.frame(age = imputed.age,
fare = imputed.fare,
pclass = titanic3$pclass)
imputed.1nn = kNN(titanic.imputed, k=1)
titanic.imputed = data.frame(age = imputed.age,
fare = imputed.fare,
pclass = titanic3$pclass)
imputed.1nn = kNN(titanic.imputed, k=1)
summary(imputed.1nn)
View(titanic.imputed)
titanic.imputed = data.frame(age = imputed.age,
fare = imputed.fare,
pclass = titanic3$pclass)
titanic.1NN = kNN(rbind(titanic.imputed, new.people), k = 1)
titanic.imputed = data.frame(age = imputed.age,
fare = imputed.fare,
pclass = titanic3$pclass)
titanic.1NN = kNN(rbind(titanic.imputed, new.people), k = 1)
summary(titanic.1NN)
titanic.imputed = data.frame(age = imputed.age,
fare = imputed.fare,
pclass = titanic3$pclass)
titanic.1NN = kNN(rbind(titanic.imputed, new.people), k = 1)
summary(titanic.1NN)
View(titanic.1NN)
knn = sqrt(nrow(titanic.imputed))
titanic.kNN = kNN(rbind(titanic.imputed, new.people), k = knn)
View(titanic.kNN)
View(titanic.imputed)
View(caret.titanic)
titanic4 = titanic3[,-c(3, 8:14)]
View(titanic4)
titanic4 = titanic3[,-c(3, 8:14)]
titanic4$fare = imputed.fare
titanic_complete = titanic4[,complete.cases(titanic4)]
titanic_complete = titanic4[complete.cases(titanic4)]
titanic_complete = titanic4[complete.cases(titanic4),:]
titanic_complete = titanic4[complete.cases(titanic4),]
titanic_complete = titanic4[complete.cases(titanic4),]
titanic_incomplete = titanic4[!complete.cases(titanic4), -4]
library(kknn)
kknn(titanic4 ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 1)
library(kknn)
kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 1)
kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 2)
kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 10)
library(kknn)
manhattan = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 1)
euclidean = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 2)
minkowski = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 10)
library(kknn)
manhattan = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 1)
euclidean = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 2)
minkowski = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 10)
summary(manhattan)
summary(euclidean)
summary(minkowsi)
library(kknn)
manhattan = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 1)
euclidean = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 2)
minkowski = kknn(age ~ ., titanic_complete, titanic_incomplete, k = 1, distance = 10)
summary(manhattan)
summary(euclidean)
summary(minkowski)
View(manhattan)
View(manhattan)
plot(density(manhattan$fitted.values), col = "red",
main = "Minkowski Imputation for Age\n1 Nearest Neighbor")
lines(density(euclidean$fitted.values), col = "green")
lines(density(minkowski10$fitted.values), col = "blue")
plot(density(manhattan$fitted.values), col = "red",
main = "Minkowski Imputation for Age\n1 Nearest Neighbor")
lines(density(euclidean$fitted.values), col = "green")
lines(density(minkowski$fitted.values), col = "blue")
lines(density(titanic3$age, na.rm = TRUE), col = "purple", lty = 2)
legend("topright", c("Manhattan", "Euclidean", "p = 10", "Original"),
col = c("red", "green", "blue", "purple"), lwd = 1, lty = c(1, 1, 1, 2))
manhattan = kknn(age ~ ., titanic_complete, titanic_incomplete, k = knn, distance = 1)
euclidean = kknn(age ~ ., titanic_complete, titanic_incomplete, k = knn, distance = 2)
minkowski = kknn(age ~ ., titanic_complete, titanic_incomplete, k = knn, distance = 10)
plot(density(manhattan$fitted.values), col = "red",
main = "Minkowski Imputation for Age\n1 Nearest Neighbor")
lines(density(euclidean$fitted.values), col = "green")
lines(density(minkowski$fitted.values), col = "blue")
lines(density(titanic3$age, na.rm = TRUE), col = "purple", lty = 2)
legend("topright", c("Manhattan", "Euclidean", "p = 10", "Original"),
col = c("red", "green", "blue", "purple"), lwd = 1, lty = c(1, 1, 1, 2))
